{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Function to get and parse the HTML content\n",
        "def get_parsed_html(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "# Function to extract the title of the Wikipedia page\n",
        "def extract_title(soup):\n",
        "    title = soup.find(id=\"firstHeading\").get_text()\n",
        "    return title\n",
        "\n",
        "# Function to extract the content of the Wikipedia page\n",
        "def extract_content(soup):\n",
        "    content = {}\n",
        "    current_heading = None  # To keep track of the current header (h2, h3)\n",
        "\n",
        "    # Find headings and paragraphs\n",
        "    for element in soup.find_all(['h2', 'h3', 'p']):\n",
        "        if element.name in ['h2', 'h3']:\n",
        "            current_heading = element.get_text().strip()  # Set the current heading\n",
        "            content[current_heading] = []  # Initialize the list of paragraphs for this heading\n",
        "        elif element.name == 'p' and current_heading:\n",
        "            # Add the paragraph under the current heading\n",
        "            paragraph = element.get_text().strip()\n",
        "            if paragraph:  # Ensure it's not an empty paragraph\n",
        "                content[current_heading].append(paragraph)\n",
        "\n",
        "    return content\n",
        "\n",
        "# Function to extract all Wikipedia links from the page\n",
        "def extract_wikipedia_links(soup):\n",
        "    links = []\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        if href.startswith('/wiki/') and not ':' in href:\n",
        "            full_url = f\"https://en.wikipedia.org{href}\"\n",
        "            links.append(full_url)\n",
        "    return links\n",
        "\n",
        "# Main function to scrape the data and write to a CSV file\n",
        "def scrape_wikipedia_to_csv(url, filename):\n",
        "    # Get the parsed HTML content of the page\n",
        "    soup = get_parsed_html(url)\n",
        "\n",
        "    # Extract the title, content, and links\n",
        "    title = extract_title(soup)\n",
        "    content = extract_content(soup)\n",
        "    links = extract_wikipedia_links(soup)\n",
        "\n",
        "    # Write the extracted data to a CSV file\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the title as the first row\n",
        "        writer.writerow(['Title', title])\n",
        "        writer.writerow([])  # Empty row for separation\n",
        "\n",
        "        # Write the headings and their respective paragraphs\n",
        "        writer.writerow(['Heading', 'Paragraphs'])\n",
        "        for heading, paragraphs in content.items():\n",
        "            for paragraph in paragraphs:\n",
        "                writer.writerow([heading, paragraph])\n",
        "\n",
        "        writer.writerow([])  # Empty row for separation\n",
        "\n",
        "        # Write the links\n",
        "        writer.writerow(['Wikipedia Links'])\n",
        "        for link in links:\n",
        "            writer.writerow([link])\n",
        "\n",
        "# Function to display the contents of the CSV file\n",
        "def display_csv_content(filename):\n",
        "    with open(filename, mode='r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        for row in reader:\n",
        "            print(row)\n",
        "\n",
        "# Example usage: scrape the \"Web scraping\" Wikipedia page and display the CSV content\n",
        "scrape_wikipedia_to_csv(\"https://en.wikipedia.org/wiki/Web_scraping\", \"wikipedia_data.csv\")\n",
        "display_csv_content(\"wikipedia_data.csv\")\n"
      ],
      "metadata": {
        "id": "IJiOvuIq5dmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests #sending https requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#function to parse html of the web page\n",
        "def getpage (link, timeout):\n",
        "  attempts = 1\n",
        "  maxattempts = timeout * 60 #set max num of attemptd in seconds (timeout is in min, *60)\n",
        "  while True:\n",
        "    try:\n",
        "      page = requests.get(link)\n",
        "      soup = BeautifulSoup(page.content, 'html.parser')\n",
        "      print(\"sucessfully connected\")\n",
        "      return soup\n",
        "    #if connection error or timeout occurs, catch these exception\n",
        "    except (requests.ConnectionError, requests.ConnectTimeOut) as exp:\n",
        "      print(\"error\")\n",
        "      attempts+=1\n",
        "      #check if the number of attemptd had exceeded maxattempts print errors\n",
        "      if attempts <= maxattempts:\n",
        "        continue\n",
        "      else:\n",
        "        print(\"error\")\n",
        "        return None\n",
        "link = \"https://en.wikipedia.org/wiki/Python_\"\n",
        "mins= 1\n",
        "getpage(link,mins)"
      ],
      "metadata": {
        "id": "RnhW41XY_AbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WEmc3-QHF-4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://datatables.net/examples/data_sources/dom.html'\n",
        "req = requests.get(link)\n",
        "soup = BeautifulSoup(req.content, 'html.parser')\n",
        "#find the table element content using its tag\n",
        "table = soup.find('table',{'id':'example'})\n",
        "#create an empty list to store the rows\n",
        "data = []\n",
        "\n",
        "#find all rows within the table body\n",
        "for row in table.tbody.find_all('tr'):\n",
        "  cols = row.find_all('td') #get table data\n",
        "  cols = [col.text.strip() for col in cols]#extraxt the text from each cell\n",
        "  data.append(cols)\n",
        "\n",
        "df = pd.DataFrame(data, columns = ['Name', 'Position', 'Office', 'Age', 'Start date', 'Salary'])\n",
        "sort = df.sort_values(by='Name')\n",
        "print(sort)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqe9UGJcGNGF",
        "outputId": "1c81cfbb-e0d5-430f-ec80-76c431411261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   Name                       Position         Office Age  \\\n",
            "4            Airi Satou                     Accountant          Tokyo  33   \n",
            "24       Angelica Ramos  Chief Executive Officer (CEO)         London  47   \n",
            "2            Ashton Cox        Junior Technical Author  San Francisco  66   \n",
            "18        Bradley Greer              Software Engineer         London  41   \n",
            "27       Brenden Wagner              Software Engineer  San Francisco  28   \n",
            "5    Brielle Williamson         Integration Specialist       New York  61   \n",
            "42           Bruno Nash              Software Engineer         London  38   \n",
            "22         Caesar Vance              Pre-Sales Support       New York  21   \n",
            "50         Cara Stevens                Sales Assistant       New York  46   \n",
            "3          Cedric Kelly    Senior Javascript Developer      Edinburgh  22   \n",
            "12      Charde Marshall              Regional Director  San Francisco  36   \n",
            "8         Colleen Hurst           Javascript Developer  San Francisco  39   \n",
            "19             Dai Rios                 Personnel Lead      Edinburgh  35   \n",
            "56         Donna Snider               Customer Support       New York  27   \n",
            "23         Doris Wilder                Sales Assistant         Sydney  23   \n",
            "45         Finn Camacho               Support Engineer  San Francisco  47   \n",
            "28          Fiona Green  Chief Operating Officer (COO)  San Francisco  48   \n",
            "1       Garrett Winters                     Accountant          Tokyo  63   \n",
            "33         Gavin Cortez                    Team Leader  San Francisco  22   \n",
            "25          Gavin Joyce                      Developer      Edinburgh  42   \n",
            "17        Gloria Little          Systems Administrator       New York  59   \n",
            "13        Haley Kennedy      Senior Marketing Designer         London  43   \n",
            "51      Hermione Butler              Regional Director         London  47   \n",
            "6       Herrod Chandler                Sales Assistant  San Francisco  59   \n",
            "37         Hope Fuentes                      Secretary  San Francisco  41   \n",
            "36      Howard Hatfield                 Office Manager  San Francisco  51   \n",
            "40     Jackson Bradshaw                       Director       New York  65   \n",
            "10          Jena Gaines                 Office Manager         London  30   \n",
            "20     Jenette Caldwell               Development Lead       New York  30   \n",
            "49      Jennifer Acosta    Junior Javascript Developer      Edinburgh  43   \n",
            "26       Jennifer Chang              Regional Director      Singapore  28   \n",
            "53      Jonas Alexander                      Developer  San Francisco  30   \n",
            "52           Lael Greer          Systems Administrator         London  21   \n",
            "34       Martena Mccray             Post-Sales support      Edinburgh  46   \n",
            "55        Michael Bruce           Javascript Developer      Singapore  29   \n",
            "15        Michael Silva             Marketing Designer         London  66   \n",
            "30       Michelle House         Integration Specialist         Sydney  37   \n",
            "41         Olivia Liang               Support Engineer      Singapore  64   \n",
            "16            Paul Byrd  Chief Financial Officer (CFO)       New York  64   \n",
            "32    Prescott Bartlett               Technical Author         London  27   \n",
            "11          Quinn Flynn                   Support Lead      Edinburgh  22   \n",
            "7        Rhona Davidson         Integration Specialist          Tokyo  55   \n",
            "43      Sakura Yamamoto               Support Engineer          Tokyo  37   \n",
            "46        Serge Baldwin               Data Coordinator      Singapore  64   \n",
            "54          Shad Decker              Regional Director      Edinburgh  51   \n",
            "29            Shou Itou             Regional Marketing          Tokyo  20   \n",
            "9           Sonya Frost              Software Engineer      Edinburgh  23   \n",
            "31           Suki Burks                      Developer         London  53   \n",
            "14  Tatyana Fitzpatrick              Regional Director         London  19   \n",
            "44          Thor Walton                      Developer       New York  61   \n",
            "0           Tiger Nixon               System Architect      Edinburgh  61   \n",
            "39       Timothy Mooney                 Office Manager         London  37   \n",
            "35         Unity Butler             Marketing Designer  San Francisco  47   \n",
            "38       Vivian Harrell           Financial Controller  San Francisco  62   \n",
            "21           Yuri Berry  Chief Marketing Officer (CMO)       New York  40   \n",
            "47        Zenaida Frank              Software Engineer       New York  63   \n",
            "48       Zorita Serrano              Software Engineer  San Francisco  56   \n",
            "\n",
            "    Start date      Salary  \n",
            "4   2008-11-28    $162,700  \n",
            "24  2009-10-09  $1,200,000  \n",
            "2   2009-01-12     $86,000  \n",
            "18  2012-10-13    $132,000  \n",
            "27  2011-06-07    $206,850  \n",
            "5   2012-12-02    $372,000  \n",
            "42  2011-05-03    $163,500  \n",
            "22  2011-12-12    $106,450  \n",
            "50  2011-12-06    $145,600  \n",
            "3   2012-03-29    $433,060  \n",
            "12  2008-10-16    $470,600  \n",
            "8   2009-09-15    $205,500  \n",
            "19  2012-09-26    $217,500  \n",
            "56  2011-01-25    $112,000  \n",
            "23  2010-09-20     $85,600  \n",
            "45  2009-07-07     $87,500  \n",
            "28  2010-03-11    $850,000  \n",
            "1   2011-07-25    $170,750  \n",
            "33  2008-10-26    $235,500  \n",
            "25  2010-12-22     $92,575  \n",
            "17  2009-04-10    $237,500  \n",
            "13  2012-12-18    $313,500  \n",
            "51  2011-03-21    $356,250  \n",
            "6   2012-08-06    $137,500  \n",
            "37  2010-02-12    $109,850  \n",
            "36  2008-12-16    $164,500  \n",
            "40  2008-09-26    $645,750  \n",
            "10  2008-12-19     $90,560  \n",
            "20  2011-09-03    $345,000  \n",
            "49  2013-02-01     $75,650  \n",
            "26  2010-11-14    $357,650  \n",
            "53  2010-07-14     $86,500  \n",
            "52  2009-02-27    $103,500  \n",
            "34  2011-03-09    $324,050  \n",
            "55  2011-06-27    $183,000  \n",
            "15  2012-11-27    $198,500  \n",
            "30  2011-06-02     $95,400  \n",
            "41  2011-02-03    $234,500  \n",
            "16  2010-06-09    $725,000  \n",
            "32  2011-05-07    $145,000  \n",
            "11  2013-03-03    $342,000  \n",
            "7   2010-10-14    $327,900  \n",
            "43  2009-08-19    $139,575  \n",
            "46  2012-04-09    $138,575  \n",
            "54  2008-11-13    $183,000  \n",
            "29  2011-08-14    $163,000  \n",
            "9   2008-12-13    $103,600  \n",
            "31  2009-10-22    $114,500  \n",
            "14  2010-03-17    $385,750  \n",
            "44  2013-08-11     $98,540  \n",
            "0   2011-04-25    $320,800  \n",
            "39  2008-12-11    $136,200  \n",
            "35  2009-12-09     $85,675  \n",
            "38  2009-02-14    $452,500  \n",
            "21  2009-06-25    $675,000  \n",
            "47  2010-01-04    $125,250  \n",
            "48  2012-06-01    $115,000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_parsed_html(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  return soup\n",
        "\n",
        "#Function to extreact a specific table by its index\n",
        "def extract_table(soup, table_index):\n",
        "  tables = soup.find_all('table')#finds all 'table' elements in the parsed html\n",
        "  if len(tables) > table_index:#check if the requested table index exists within the list\n",
        "    table = tables[table_index]#selects the tables based in the index provided\n",
        "    return table\n",
        "  else:\n",
        "    print(f\"table index {table_index} is out of range\")\n",
        "    return None\n",
        "\n",
        "#Function to convert a beautifulsoup table to df\n",
        "def table_to_df(table):\n",
        "#initialise an empty list to store table rows\n",
        "  rows = []\n",
        "#initialise an empty list to store table column headers\n",
        "  headers = []\n",
        "#get headers if ther exist\n",
        "  header_row = table.find_all('th') #find all 'th' (table header')elements in the table\n",
        "  if header_row:#if headers are found\n",
        "    headers = [header.get_text().strip() for header in header_row]#extract text from each header and add it to the list\n",
        "#get all rows\n",
        "  for tr in table.find_all('tr'): #iiterates through each 'tr'(table row) in the table\n",
        "    cells = tr.find_all(['td', 'th']) #find all 'td'(table data) or 'th'(table headers) in the table rows\n",
        "    row = [cell. get_text().strip() for cell in cells] #exctract and strp text for each cell\n",
        "#adjust rows to match the header length\n",
        "#if the row has fewer cols than the headers\n",
        "    if len(row)< len(headers):\n",
        "      row.extend([None]*(len(headers)-len(row)))# fill the missing cols with None\n",
        "    elif len(row)> len(headers):\n",
        "      row = row[:len(headers)] #cut the row to match the number of the headers\n",
        "    rows.append(row)\n",
        "#convert to df\n",
        "  if headers: #exit\n",
        "    df=pd.DataFrame(rows, columns=headers)\n",
        "  else:\n",
        "    df=pd.DataFrame(rows)\n",
        "  return df\n",
        "\n",
        "#main function to scrape and display a specific table\n",
        "def display_specific_table(url, table_index):\n",
        "  soup = get_parsed_html(url)\n",
        "  table = extract_table(soup, table_index)\n",
        "\n",
        "  if table: #found\n",
        "    df = table_to_df(table) #convert the table to df\n",
        "    print(df)\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
        "table_index = 2\n",
        "display_specific_table (url, table_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVfcrVd_MiUq",
        "outputId": "99a949a1-3c8e-42e4-f877-06c3888deb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Country/Territory   IMF[1][13]  World Bank[14]  United Nations[15]  \\\n",
            "0    Country/Territory   IMF[1][13]  World Bank[14]  United Nations[15]   \n",
            "1             Forecast         Year        Estimate                Year   \n",
            "2                World  109,529,216            2024         105,435,540   \n",
            "3        United States   28,781,083            2024          27,360,935   \n",
            "4                China   18,532,633       [n 1]2024          17,794,782   \n",
            "..                 ...          ...             ...                 ...   \n",
            "207           Kiribati          311            2024                 279   \n",
            "208              Palau          308            2024                 263   \n",
            "209   Marshall Islands          305            2024                 284   \n",
            "210              Nauru          161            2024                 154   \n",
            "211             Tuvalu           66            2024                  62   \n",
            "\n",
            "      Forecast         Year   Estimate  Year Estimate  Year  \n",
            "0         None         None       None  None     None  None  \n",
            "1     Estimate         Year       None  None     None  None  \n",
            "2         2023  100,834,796       2022  None     None  None  \n",
            "3         2023   25,744,100       2022  None     None  None  \n",
            "4    [n 3]2023   17,963,170  [n 1]2022  None     None  None  \n",
            "..         ...          ...        ...   ...      ...   ...  \n",
            "207       2023          223       2022  None     None  None  \n",
            "208       2023          225       2022  None     None  None  \n",
            "209       2023          279       2022  None     None  None  \n",
            "210       2023          147       2022  None     None  None  \n",
            "211       2023           59       2022  None     None  None  \n",
            "\n",
            "[212 rows x 10 columns]\n"
          ]
        }
      ]
    }
  ]
}